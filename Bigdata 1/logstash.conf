input {
  file {
    # Lire tous les fichiers CSV compressés .gz dans le répertoire
    mode => "tail"  # Assurez-vous que c'est adapté à votre cas d'utilisation
    path => "/usr/share/logstash/ingest_data/*.csv.gz"  # Modifiez le chemin pour cibler spécifiquement les fichiers .gz
    sincedb_path => "/dev/null"  # Ignore l'état de lecture pour les tests
  }
}

filter {
  # Décompression et traitement des fichiers CSV
  csv {
    separator => ","  # Spécifiez le séparateur du fichier CSV
    # Liste des colonnes à extraire
    columns => [
      "id", "number", "street_name", "zipcode", "city", "source", "latitude", "longitude"
    ]
    remove_field => ["message", "@version", "@timestamp", "host"]  # Supprime les champs non nécessaires
  }

  mutate {
    # Conversion des champs de longitude et latitude en float
    convert => {
      "longitude" => "float"
      "latitude" => "float"
    }
    # Renommage des champs pour une meilleure structuration
    rename => {
      "longitude" => "[location][lon]"
      "latitude" => "[location][lat]"
      "number" => "[address][number]"
      "street_name" => "[address][street_name]"
      "zipcode" => "[address][zipcode]"
      "city" => "[address][city]"
    }
    # Si vous utilisez une variable d'environnement REGION
    replace => {
      "region" => "${REGION}"  # Assurez-vous que cette variable est définie dans votre environnement
    }
  }
}

output {
  elasticsearch {
    index => "logstash-%{+YYYY.MM.dd}"  # Index par date
    hosts => "${ELASTIC_HOSTS}"  # Variable d'environnement pour les hôtes Elasticsearch
    user => "${ELASTIC_USER}"  # Utilisateur pour se connecter à Elasticsearch
    password => "${ELASTIC_PASSWORD}"  # Mot de passe pour se connecter à Elasticsearch
    cacert => "certs/ca/ca.crt"  # Certificat CA pour la connexion sécurisée
    region => "${REGION:fr}" # Définit une valeur par défaut ici
  }
}
